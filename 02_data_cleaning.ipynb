{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b7f73a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from Pickle Files/step1.pkl: (10000, 9)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load data from previous notebook using pickle\n",
    "with open('Pickle Files/step1.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "    \n",
    "print(\"Data loaded from Pickle Files/step1.pkl:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c482e7",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "To complete the cleaning process for e-commerce user event data, perform the following steps:\n",
    "\n",
    "- Handle missing values (impute or drop as appropriate)\n",
    "\n",
    "- Remove duplicate records (IQR, z-score)\n",
    "\n",
    "- Detect and address outliers\n",
    "\n",
    "- Validate data types and ranges\n",
    "\n",
    "- Document any cleaning decisions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ffd87b",
   "metadata": {},
   "source": [
    "## Strategies for Handling Missing Values\n",
    "After visualizing missing data, we can address it by either imputing (filling in) missing values or dropping rows/columns with missing data. The choice depends on the amount and importance of missing data:\n",
    "\n",
    "- **Imputation:** Fill missing values using mean, median, mode, or other methods.\n",
    "- **Dropping:** Remove rows or columns with excessive missing values if they are not critical for analysis.\n",
    "\n",
    "Careful handling ensures data quality and reliable analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ebe343d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after cleaning: (10000, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "event_time          0\n",
       "event_type          0\n",
       "product_id          0\n",
       "category_id         0\n",
       "category_code    3277\n",
       "brand            1442\n",
       "price               0\n",
       "user_id             0\n",
       "user_session        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handle missing values: drop rows with >50% missing, impute remaining with mean (numeric columns)\n",
    "threshold = int(df.shape[1] * 0.5)\n",
    "df = df.dropna(thresh=threshold)\n",
    "\n",
    "# Impute remaining missing values in numeric columns with mean\n",
    "for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "print('Shape after cleaning:', df.shape)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "882e4e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 3 duplicate rows. Remaining rows: 9997\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "before = df.shape[0]\n",
    "df = df.drop_duplicates()\n",
    "after = df.shape[0]\n",
    "print(f\"Removed {before - after} duplicate rows. Remaining rows: {after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d92b90",
   "metadata": {},
   "source": [
    "## What are Outliers?\n",
    "\n",
    "Outliers are data points that deviate significantly from other observations in your dataset.\n",
    "\n",
    "They can occur due to:\n",
    "- Data entry errors\n",
    "- Measurement errors\n",
    "- Natural variability (some users spend much more than others)\n",
    "\n",
    "**Examples in your dataset:**\n",
    "- Extremely high-priced items (price column)\n",
    "- Unusually high number of purchases by a single user\n",
    "\n",
    "**Why it matters for clustering:**\n",
    "Clustering algorithms like K-Means are distance-based. Outliers can skew centroids, leading to poor cluster definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b4f861f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique outliers across all numeric columns: 1461\n",
      "Shape after removing outliers: (8536, 9)\n"
     ]
    }
   ],
   "source": [
    "# Detect and remove outliers using the IQR method for all numeric columns (collect indices and remove all at once)\n",
    "import numpy as np\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "outlier_index = set()\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outlier_index.update(df[(df[col] < lower_bound) | (df[col] > upper_bound)].index)\n",
    "print(f\"Total unique outliers across all numeric columns: {len(outlier_index)}\")\n",
    "df = df.drop(outlier_index)\n",
    "print(f\"Shape after removing outliers: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c8b69",
   "metadata": {},
   "source": [
    "## Validating Data Types and Value Ranges\n",
    "\n",
    "After handling missing values, duplicates, and outliers, it's important to ensure that each column has the correct data type and that values fall within expected ranges. This step helps prevent errors in downstream analysis and modeling.\n",
    "print(\"Data types before correction:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "for col in df.select_dtypes(include=[np.number]).columns:\n",
    "    min_val = df[col].min()\n",
    "    max_val = df[col].max()\n",
    "    print(f\"{col}: min={min_val}, max=`{max_val}\")\n",
    "\n",
    "print(\"\\nData types after correction:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ef74630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate and correct data types\n",
    "if 'event_time' in df.columns:\n",
    "    df['event_time'] = pd.to_datetime(df['event_time'], errors='coerce')\n",
    "if 'event_type' in df.columns:\n",
    "    df['event_type'] = df['event_type'].astype(str)\n",
    "if 'product_id' in df.columns:\n",
    "    df['product_id'] = df['product_id'].astype(str)\n",
    "if 'category_id' in df.columns:\n",
    "    df['category_id'] = df['category_id'].astype('category')\n",
    "if 'category_code' in df.columns:\n",
    "    df['category_code'] = df['category_code'].astype(str)\n",
    "if 'brand' in df.columns:\n",
    "    df['brand'] = df['brand'].astype(str)\n",
    "if 'price' in df.columns:\n",
    "    df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "if 'user_id' in df.columns:\n",
    "    df['user_id'] = df['user_id'].astype(str)\n",
    "if 'user_session' in df.columns:\n",
    "    df['user_session'] = df['user_session'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435fb5bf",
   "metadata": {},
   "source": [
    "All Data Type Corrections Applied:-\n",
    "\n",
    "The following columns were converted to their appropriate types for analysis:\n",
    "\n",
    "- `event_time`: converted to datetime\n",
    "- `event_type`: converted to string\n",
    "- `product_id`: converted to string\n",
    "- `category_id`: converted to category\n",
    "- `category_code`: converted to string\n",
    "- `brand`: converted to string\n",
    "- `price`: converted to numeric\n",
    "- `user_id`: converted to string\n",
    "- `user_session`: converted to string\n",
    "\n",
    "These conversions ensure that each column is interpreted correctly for downstream analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b939583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved as Pickle Files/step2.pkl: (8536, 9)\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned data for next notebook using pickle\n",
    "import pickle\n",
    "\n",
    "with open('Pickle Files/step2.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "    \n",
    "print(\"Cleaned data saved as Pickle Files/step2.pkl:\", df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
